{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b9cedaf",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "Kaylee Molin (22734429)\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b046f62",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c3d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_curve, silhouette_score, silhouette_samples\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from pathlib import Path\n",
    "from graphviz import Source\n",
    "import matplotlib.patches as patches  # extra code – for the curved arrow\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23f5f959",
   "metadata": {},
   "source": [
    "## Part 1: A model for diagnosing cancer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4a5f132",
   "metadata": {},
   "source": [
    "**Defining the functions to be used in this question.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a35284b0",
   "metadata": {},
   "source": [
    "---\n",
    "Task 1 functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pairplot(df, target_col, feature_cols):\n",
    "    '''Plot a pairplot with modified legend labels'''\n",
    "    g = sns.pairplot(df, hue=target_col, vars=feature_cols, corner=True)\n",
    "    g._legend.set_title('Class')\n",
    "    plt.show()\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d97ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(corr_matrix, figsize=(20,20)):\n",
    "    '''plot heatmap using a correaltion matrix'''\n",
    "    plt.figure(figsize=figsize)\n",
    "    heatmap = sns.heatmap(corr_matrix, cmap=\"coolwarm\", annot=True)\n",
    "    print(heatmap)\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dce3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(df, feature_list):\n",
    "    '''Drop features from specified dataframe'''\n",
    "    return df.drop(feature_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(corr_matrix, n=10):\n",
    "    '''Get top correlated pairs of features'''\n",
    "    au_corr = corr_matrix.abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(corr_matrix)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e817985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Xy(data, target_name):\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data[target_name]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, target_name, test_size=0.15):\n",
    "    '''Split data in the X_train, X_test, y_train, y_test and scale'''\n",
    "    X, y = split_Xy(data, target_name)\n",
    "\n",
    "    # Split the data into training and testing sets with 85% for training and 15% for testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=123)\n",
    "\n",
    "    # Scaling the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Convert X_train and X_test back to a pandas dataframe\n",
    "    X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c478bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_graph(accuracy_train, accuracy_test, title, xtitle=['Training Set', 'Test Set'], ylabel='Accuracy'):\n",
    "    '''Plots a bar graph with 2 columns'''\n",
    "    colors = ['Lightgray', 'Gray']\n",
    "\n",
    "    plt.bar(xtitle, [accuracy_train, accuracy_test], color=colors)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    for i, v in enumerate([accuracy_train, accuracy_test]):\n",
    "        plt.text(i, v + 0.01, str(round(v, 4)), color='black', ha='center')\n",
    "\n",
    "    plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2929c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_clf(clf, param_grid, X_train, y_train, X_test, y_test, model_name):\n",
    "    # Create the cross-validator object with random_state\n",
    "    cv = KFold(n_splits=3, random_state=21, shuffle=True)\n",
    "\n",
    "    # Perform grid search to find the best hyperparameters\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=cv)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best hyperparameters found\n",
    "    print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "    # Train the decision tree classifier with the best hyperparameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the training set\n",
    "    accuracy_train = best_model.score(X_train, y_train)\n",
    "    print(\"\\nAccuracy of the {} model (training): {:.2f}%\".format(model_name, accuracy_train * 100))\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    accuracy_test = best_model.score(X_test, y_test)\n",
    "    print(\"Accuracy of the {} model (testing): {:.2f}%\".format(model_name, accuracy_test * 100))\n",
    "\n",
    "\n",
    "    return accuracy_train, accuracy_test, best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_side_by_side(x_titles, accuracies_train, accuracies_test, rotate=45, title1='Training Set Accuracies', title2='Testing Set Accuracies', ylabel='Accuracies',ylim=1.05):\n",
    "    '''Plots 2 bar graphs side-by-side'''\n",
    "    colors = ['pink', 'lavender', 'lightblue', 'palegreen', 'yellow']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10,5))\n",
    "    axes[0].bar(x_titles, accuracies_train, color=colors)\n",
    "    axes[0].set_title(title1)\n",
    "    axes[0].set_ylabel(ylabel)\n",
    "    axes[0].tick_params(axis='x', labelrotation=rotate) # rotate x-tick labels\n",
    "    axes[0].set_ylim([0, ylim])\n",
    "    for i, v in enumerate(accuracies_train):\n",
    "        axes[0].text(i, v + 0.01, str(round(v, 4)), color='black', ha='center')\n",
    "\n",
    "    # Plot the testing set accuracies\n",
    "    axes[1].bar(x_titles, accuracies_test, color=colors)\n",
    "    axes[1].set_title(title2)\n",
    "    #axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].tick_params(axis='x', labelrotation=rotate) # rotate x-tick labels\n",
    "    axes[1].set_ylim([0, ylim])\n",
    "    for i, v in enumerate(accuracies_test):\n",
    "        axes[1].text(i, v + 0.01, str(round(v, 4)), color='black', ha='center')\n",
    "\n",
    "    # Display the plots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40921a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, y_test_pred, label, ax):\n",
    "    # Calculate the confusion matrix for the test set\n",
    "    cm_best = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    # Extract TN, FP, FN, TP values from the confusion matrix\n",
    "    tn_best, fp_best, fn_best, tp_best = cm_best.ravel()\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    sns.heatmap(cm_best, annot=True, fmt='d', cmap=\"Purples\", annot_kws={\"size\": 18},\n",
    "                cbar=False, square=True, xticklabels=['Predicted: M', 'Predicted: B'], \n",
    "                yticklabels=['Actual: M', 'Actual: B'], ax=ax)\n",
    "    ax.text(0.5, 1.1, f\"TN={tn_best}\\nFP={fp_best}\\nFN={fn_best}\\nTP={tp_best}\", \n",
    "    fontsize=12, ha=\"center\", transform=ax.transAxes)\n",
    "    ax.set_xlabel(\"Predicted labels\")\n",
    "    ax.set_ylabel(\"True labels\")\n",
    "    ax.set_title(str(label))\n",
    "    \n",
    "    return tn_best, fp_best, fn_best, tp_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d27e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_test, y_test_pred, label):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_test_pred)\n",
    "\n",
    "    idx = (thresholds >= 0.5).argmax()  # first index ≥ threshold\n",
    "\n",
    "    plt.plot(recalls, precisions, '--', linewidth=2, label=\"Precision/Recall curve for \"+label)\n",
    "\n",
    "    # extra code – just beautifies and saves Figure 3–6\n",
    "    #plt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], \"k:\")\n",
    "    #plt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], \"k:\")\n",
    "    #plt.plot([recalls[idx]], [precisions[idx]], \"ko\",\n",
    "             #label=\"Point at threshold 0.5\")\n",
    "    plt.gca().add_patch(patches.FancyArrowPatch(\n",
    "        (0.79, 0.60), (0.61, 0.78),\n",
    "        connectionstyle=\"arc3,rad=.2\",\n",
    "        arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n",
    "        color=\"#444444\"))\n",
    "    plt.text(0.56, 0.82, \"Higher\\nthreshold\", color=\"#333333\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.axis([0, 1.01, 0.5, 1.01])\n",
    "    plt.grid()\n",
    "    plt.legend(loc=\"lower left\")\n",
    "\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabae491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_threshold(y_test, y_pred, title, threshold=0.5):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.vlines(threshold, 0, 1.2, \"k\", \"dotted\", label=\"Threshold\")\n",
    "\n",
    "    idx = (thresholds >= threshold).argmax()  # first index ≥ threshold\n",
    "    plt.axis([0, 1, 0, 1.1])\n",
    "    plt.grid()\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"center right\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05a42799",
   "metadata": {},
   "source": [
    "---\n",
    "Task 2 functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(trained_model, data, title, threshold=0.05):\n",
    "    if hasattr(trained_model, 'feature_importances_'):\n",
    "        # Decision Tree or Random Forest\n",
    "        importances = trained_model.feature_importances_\n",
    "        sorted_idx = importances.argsort()\n",
    "        plt.barh(range(len(importances)), importances[sorted_idx])\n",
    "        for i, v in enumerate(importances[sorted_idx]):\n",
    "            plt.text(v + 0.01, i, str(round(v, 3)))\n",
    "        plt.yticks(range(len(importances)), data.columns[sorted_idx])\n",
    "        plt.xlabel('Feature Importance')\n",
    "    elif hasattr(trained_model, 'coef_'):\n",
    "        # Logistic Regression\n",
    "        coefficients = np.abs(trained_model.coef_[0])\n",
    "        coef_sum = np.sum(coefficients)\n",
    "        coefficients = coefficients/coef_sum\n",
    "        sorted_idx = np.abs(coefficients).argsort()\n",
    "        plt.barh(range(len(coefficients)), coefficients[sorted_idx])\n",
    "        for i, v in enumerate(coefficients[sorted_idx]):\n",
    "            plt.text(v + 0.01, i, str(round(v, 3)))\n",
    "        plt.yticks(range(len(coefficients)), data.columns[sorted_idx])\n",
    "        plt.xlabel('Absolute Coefficient')\n",
    "    else:\n",
    "        raise ValueError('The model does not support feature importances or coefficients.')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ec72553",
   "metadata": {},
   "source": [
    "---\n",
    "Task 3 functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_plot(X_pca, pca, y):\n",
    "\n",
    "    # Calculate the variance ratio explained by the first two principal components\n",
    "    variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "    cmap = ListedColormap(['hotpink', 'cornflowerblue'])\n",
    "\n",
    "    # Create a scatter plot of the data on the first two principal components\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=cmap, alpha=0.5)\n",
    "    plt.xlabel('1st Principal Component')\n",
    "    plt.ylabel('2nd Principal Component')\n",
    "    plt.title('PCA Scatter Plot of Breast Cancer Dataset')\n",
    "\n",
    "    # force the ticks\n",
    "    cb = plt.colorbar(ticks=[0.25,.75])\n",
    "    cb.set_ticklabels([0,1])\n",
    "    cb.ax.set_yticklabels(['0 - Benign', '1 - Malignant'])\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Proportion of data variance explained by the first two principal components: {sum(variance_ratio):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f89dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_plot_biplot(X_pca, feature_names, y, pca):\n",
    "\n",
    "    cmap = ListedColormap(['hotpink', 'cornflowerblue'])\n",
    "    \n",
    "    # Create a biplot with variable vectors\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=cmap, alpha=0.5)\n",
    "\n",
    "    # Plot variable vectors with loadings and longer lines\n",
    "    feature_vectors = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        loadings_text = f'{feature} ({feature_vectors[i, 0]:.2f}, {feature_vectors[i, 1]:.2f})'\n",
    "        ax.arrow(0, 0, feature_vectors[i, 0]*4, feature_vectors[i, 1]*4, color='black', alpha=0.7, linewidth=2)\n",
    "        ax.text(feature_vectors[i, 0] * 4.2, feature_vectors[i, 1] * 4.2, loadings_text, color='black', fontweight='bold')\n",
    "\n",
    "    ax.set_xlabel('1st Principal Component')\n",
    "    ax.set_ylabel('2nd Principal Component')\n",
    "    ax.set_title('Biplot of Data on the First Two Principal Components')\n",
    "\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    ax.axvline(0, color='black', linewidth=0.5)\n",
    "\n",
    "    cbar = plt.colorbar(scatter, ax=ax, ticks=[0.25, 0.75])\n",
    "    cbar.set_ticklabels([0, 1])\n",
    "    cbar.ax.set_yticklabels(['0 - Benign', '1 - Malignant'])\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c7a94af",
   "metadata": {},
   "source": [
    "---\n",
    "Task 5 functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff0f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_comps(X, variance_limit=0.95):\n",
    "    n_components=X.shape[1]\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "    # Calculate the cumulative explained variance ratio\n",
    "    explained_variance_ratio_cumulative = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "    # Find the number of components to retain 95% of the explained variance\n",
    "    num_components = np.argmax(explained_variance_ratio_cumulative >= variance_limit) + 1\n",
    "\n",
    "    # Print the explained variance ratio of each component\n",
    "    for i, ratio in enumerate(explained_variance_ratio):\n",
    "        print(f\"Component {i+1}: Explained Variance Ratio = {ratio:.4f}\")\n",
    "\n",
    "    # Print the components contributing to 95% of the explained variance\n",
    "    print(f\"\\nComponents contributing to at least {variance_limit*100:.0f}% of the explained variance: {num_components}\")\n",
    "    print(f\"\\nVariance retained using {num_components} components: {100*explained_variance_ratio_cumulative[num_components-1]:.2f}%\")\n",
    "    return num_components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "729d0414",
   "metadata": {},
   "source": [
    "## Part  - Task 1\n",
    "\n",
    "**Building a logistic regression and a decision tree model to predict the status of a tumour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a32ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "cancer_data = pd.read_csv(\"breast-cancer.csv\")\n",
    "\n",
    "# Viewing the data to look at the features\n",
    "cancer_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d40ca2a",
   "metadata": {},
   "source": [
    "> The \"id\" column is unnecessary in the model and will be removed. The \"diagnosis\" column will be set as the target. The 'M' and the 'B' labels will be replaced with 1 and 0 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the \"id\" column\n",
    "cancer_data = drop_features(cancer_data, \"id\")\n",
    "\n",
    "# Setting 'M' -> 1 and 'B' -> 0\n",
    "cancer_data['diagnosis'] = cancer_data['diagnosis'].replace({'M': 1, 'B': 0})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a303bb8",
   "metadata": {},
   "source": [
    "**Viewing correlation between features for feature reduction and to simplify the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb499302",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_cancer = cancer_data.corr()\n",
    "\n",
    "plot_heatmap(corr_matrix_cancer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f2b83e5",
   "metadata": {},
   "source": [
    "> There are many highly correlated features. Redundant features can introduce unnecessary complexity to the model and potentially lead to overfitting. These features will not be removed at this stage as PCA will identify them anyway. However, removing features can help with:\n",
    "\n",
    ">1. Multicollinearity: Highly correlated features often exhibit multicollinearity, which means they provide similar information to the model. When multicollinearity is present, it becomes difficult for the model to discern the individual contribution of each feature, potentially leading to unstable and unreliable results.\n",
    "\n",
    ">2. Simplifying the model: By removing redundant features, you simplify the model and reduce its complexity. It becomes easier to understand the impact of each remaining feature on the model's predictions and reduces computational time.\n",
    "\n",
    ">3. Better generalisation: Removing redundant features can help prevent overfitting. By reducing the number of correlated features, the model focuses on the essential information, allowing it to generalise better and make more accurate predictions on unseen data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e59194a3",
   "metadata": {},
   "source": [
    "**Determining the size of the dataset and if it is balanced or not.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "009e8ea5",
   "metadata": {},
   "source": [
    "> The Wisconsin Breast Cancer dataset contains a total of 569 samples, which is relatively small in terms of sample size. When building a model for predicting cancer diagnosis, it is crucial to have both high accuracy and high sensitivity. Given the limited size of this dataset, it may pose challenges in achieving the necessary model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6331f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of instances for each class label\n",
    "diagnosis_counts = cancer_data['diagnosis'].value_counts()\n",
    "print(diagnosis_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b591a6da",
   "metadata": {},
   "source": [
    "> This dataset is imbalanced, with 357 benign diagnoses and 212 malignant diagnoses. The dataset is skewed towards the benign class. Imbalanced datasets can affect the model's ability to accurately predict the minority class (in this case, malignant samples). Since the number of malignant samples is relatively smaller, the model may tend to favour predicting the majority class (benign) more frequently. This needs to be considered when assessing the model's performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ea6bc05",
   "metadata": {},
   "source": [
    "**Scaling the data and preprocessing it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move 'diagnosis' column to the end\n",
    "#cancer_data_dropped = cancer_data_dropped.reindex(columns=[col for col in cancer_data_dropped.columns if col != 'diagnosis'] + ['diagnosis'])\n",
    "#cancer_data_dropped.head()\n",
    "\n",
    "cancer_data= cancer_data.reindex(columns=[col for col in cancer_data.columns if col != 'diagnosis'] + ['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee940122",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_data(cancer_data, 'diagnosis', test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d2ceb29",
   "metadata": {},
   "source": [
    "> The dataset contains only 569 samples. Therefore, a 20% test size is chosen because it allows for a reasonable evaluation while still preserving a sufficient number of samples for training and validation purposes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb0abfc7",
   "metadata": {},
   "source": [
    "### Building a logistic regression model\n",
    "\n",
    "**Using Grid search and cross validation (implemented within `get_best_clf`) to find the best hyperparameters for the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [100, 10, 1.0, 0.1, 0.01],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'class_weight': ['balanced', None],\n",
    "    'solver': ['liblinear', 'saga'], \n",
    "    'max_iter': [10000]\n",
    "}\n",
    "\n",
    "accuracy_train_lr, accuracy_test_lr, best_lr = get_best_clf(logreg, param_grid_lr, X_train, y_train, X_test, y_test, 'logistic regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d48e18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_bar_graph(accuracy_train_lr, accuracy_test_lr, 'Tuned Logistic Regression Accuracies')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e08d7f04",
   "metadata": {},
   "source": [
    "The parameters `C`, `penalty`, `class_weight` and `solver` were used in this grid search to explore a range of possible values and combinations that could potentially optimise the performance of the model.\n",
    "\n",
    "* The `C` parameter represents the inverse of regularisation strength. Higher values of `C` correspond to less regularisation, allowing the model to potentially fit the training data more closely.\n",
    "* The `penalty` parameter determines the type of regularisation used (L1 or L2 which correspond with Lasso and Ridge regression respectively).\n",
    "* The `class_weight` parameter is used with `balanced` and `None` options to handle class imbalance in the dataset. This dataset is imbalanced, hence may be necessary to use this parameter. It allows the model to assign different weights to different classes during training to more fairly represent the classes leading to more accurate predictions.\n",
    "The `solver` parameter offers different algorithms to solve the optimisation problem. `liblinear` and `saga` are selected.\n",
    "\n",
    "> As seen in the above accuracy plot, the Logistic Regression model performs very well on both the testing and training sets, with accuracies above 93% for both. This may suggest that the Logistic Regression model is well suited for this problem. However, it is important to assess the model in a variety of ways before making conclusion on its suitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af733906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the terms from the trained logistic regression model\n",
    "weights = best_lr.coef_\n",
    "bias = best_lr.intercept_\n",
    "\n",
    "# Compute the dot product of input data with the transposed weights and add the bias term\n",
    "dots = np.dot(X_train, np.transpose(weights)) + bias\n",
    "\n",
    "## Predict the class probabilities using the logistic regression model\n",
    "y_axis = best_lr.predict_proba(X_train)\n",
    "\n",
    "# Sort the dot product values and corresponding class probabilities\n",
    "dots_sorted, y_sorted = zip(*sorted(zip(dots, y_axis)))\n",
    "\n",
    "# Extract the decision boundary\n",
    "decision_boundary = np.array(dots_sorted)[np.array(y_sorted)[:, 1] >= 0.5][0, 0]\n",
    "\n",
    "plt.plot([decision_boundary, decision_boundary], [0, 1], \"k:\", linewidth=2, label=\"Decision boundary\")\n",
    "plt.title(\"Logistic Regression for Cancer Dataset\")\n",
    "plt.plot(dots_sorted, np.array(y_sorted)[:, 0], \"b--\", linewidth=2, label=\"M\")\n",
    "plt.plot(dots_sorted, np.array(y_sorted)[:, 1], \"g-\", linewidth=2, label=\"B\") \n",
    "\n",
    "plt.arrow(x=decision_boundary, y=0.08, dx=-1, dy=0, head_width=0.05, head_length=0.2, fc=\"b\", ec=\"b\")\n",
    "plt.arrow(x=decision_boundary, y=0.92, dx=1, dy=0, head_width=0.05, head_length=0.2, fc=\"g\", ec=\"g\")\n",
    "plt.legend(loc=\"center right\")\n",
    "\n",
    "# Retrieving 10 random instances to plot from the testing set\n",
    "random.seed(21)\n",
    "random_indices = np.sort(np.array([random.randint(0, len(X_test)) for _ in range(10)]))\n",
    "\n",
    "dots_testing = (np.dot(X_test, np.transpose(weights)) + bias)[random_indices]\n",
    "y_axis_testing = (best_lr.predict_proba(X_test))[random_indices]\n",
    "y_axis_proba = (best_lr.predict(X_test))[random_indices]\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.grid()\n",
    "\n",
    "s=y_axis_testing[:,0]\n",
    "d=y_axis_testing[:,1]\n",
    "plt.axis([-6, 12, -0.02, 1.02]) # [x left lim, x right lim, y lower lim, y upper lim]\n",
    "\n",
    "plt.plot(dots_testing[y_axis_proba == 0], y_axis_proba[y_axis_proba == 0], \"bs\")\n",
    "plt.plot(dots_testing[y_axis_proba == 1], y_axis_proba[y_axis_proba == 1], \"g^\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Decision boundary for the logistic regression model: {decision_boundary:.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d67fceda",
   "metadata": {},
   "source": [
    "> The decision boundary is very close to 0 which can be advantageous. This is because the dataset aims to classify tumours as either malignant or benign based on various features. A decision boundary near 0 indicates that the model is making predictions based on a subtle balance between the two classes, potentially leading to more accurate and precise classifications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b75b4bd5",
   "metadata": {},
   "source": [
    "### Building a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a45805",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "accuracy_train_dt, accuracy_test_dt, best_dt = get_best_clf(dtree, param_grid_dt, X_train, y_train, X_test, y_test, 'decision tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca180de",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = Path() / \"images\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        pltbefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0017e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(\n",
    "        best_dt,\n",
    "        out_file=str(IMAGES_PATH / \"tree.dot\"), \n",
    "        feature_names=list(X_train.columns),\n",
    "        class_names=['malignant', 'benign'],\n",
    "        rounded=True,\n",
    "        filled=True,\n",
    "    \tmax_depth=3\n",
    "    )\n",
    "\n",
    "Source.from_file(IMAGES_PATH / \"tree.dot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9379f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_graph(accuracy_train_dt, accuracy_test_dt, 'Tuned Decision Tree Accuracies')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4d49431",
   "metadata": {},
   "source": [
    "The parameters `criterion`, `max_depth`, `min_samples_split` and `min_samples_leaf` were used in this grid search to explore a range of possible values and combinations that could potentially optimise the performance of the model.\n",
    "\n",
    "* The `criterion`: [`gini`, `entropy`]: The criterion parameter determines the quality of a split in the decision tree. By including both `gini` and `entropy`, we assess two commonly used metrics for measuring impurity, enabling us to compare their performance and select the better criterion.\n",
    "\n",
    "- `max_depth`: [None, 5, 10, 15]: The max_depth parameter defines the maximum depth of the decision tree. This allows us to avoid overfitting or underfitting and find an appropriate depth that balances complexity and performance.\n",
    "\n",
    "- `min_samples_split`: [2, 5, 10]: The min_samples_split parameter sets the minimum number of samples required to split an internal node.\n",
    "\n",
    "- `min_samples_leaf`: [1, 2, 4]: The min_samples_leaf parameter specifies the minimum number of samples required to be at a leaf node. \n",
    "\n",
    "> The accuracy of the decision tree model on the training set was very high at aroung 98%, indicating a strong ability to fit the training data. On the testing set, the model achieved an accuracy of around 90%. It is possible that this model is overfitting to the training data as seen by the high trainig set accuracy and lower testing set accuracy. Also, looking at the decision tree, some nodes have an entropy of 0 indicating a perfect separation of the data, which could suggest potential overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e61df84",
   "metadata": {},
   "source": [
    "### Comparing the Logisitc Regression Model with the Decision Tree Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aca95ba7",
   "metadata": {},
   "source": [
    "**Comparison of Accuracies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf25f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_side_by_side(['Logistic Regression', 'Decision Tree'], [accuracy_train_lr, accuracy_train_dt], [accuracy_test_lr, accuracy_test_dt])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aebf974e",
   "metadata": {},
   "source": [
    "**Comparison of Confusion Matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f333f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = best_lr.predict(X_test)\n",
    "y_pred_dt = best_dt.predict(X_test)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))  # Modified line: 1 row, 2 columns\n",
    "\n",
    "tn_best_default, fp_best_default, fn_best_default, tp_best_default = plot_confusion_matrix(y_test, y_pred_lr, \"Logistic Regression\", axs[0])\n",
    "tn_best_depth, fp_best_depth, fn_best_depth, tp_best_depth = plot_confusion_matrix(y_test, y_pred_dt, \"Decision Tree\", axs[1])\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34df2219",
   "metadata": {},
   "source": [
    "**Comparison Precision/Recall Curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall(y_test, y_pred_lr, \"Logistic Regression\")\n",
    "plot_precision_recall(y_test, y_pred_dt, \"Decision Tree\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6894643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the first subplot\n",
    "plt.sca(axes[0])\n",
    "plot_with_threshold(y_test, y_pred_lr, \"Logistic Regression\", threshold=0.84)\n",
    "\n",
    "# Plot the second subplot\n",
    "plt.sca(axes[1])\n",
    "plot_with_threshold(y_test, y_pred_dt, \"Decision Tree\", threshold=0.96)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb894bb7",
   "metadata": {},
   "source": [
    "> The objective of the model is to precisely classify tumours as either benign (B) or malignant (M) by utilizing information from the Breast Cancer Wisconsin Diagnostic Dataset. The dataset's features provide details about cell nuclei extracted from suspicious breast tissue through a fine needle. The primary purpose is to use these features to effectively identify breast cancer. The ultimate aim is to develop a dependable and precise model that can assist in the early detection and treatment of breast cancer.\n",
    "\n",
    "> Based on the accuracy on both the training and testing datasets, logistic regression achieved higher accuracy compared to the decision tree. Logistic regression achieved an accuracy of 0.9561 on the testing dataset, while the decision tree achieved an accuracy of 0.9123. Considering the nature of a clinical setting where accurate predictions are crucial, logistic regression's higher accuracy suggests that it may be more appropriate for this particular scenario.\n",
    "\n",
    "> When dealing with classification of this nature, it is very important to correctly identify all positive cases as positives. Therefore, a model with a high true positive (TP) rate and a low false negative (FN) rate is desired, even if that comes at the cost of a greater number of false positives. This is not too much of an issue, as the patient will simply undergo further tests to confirm or deny the positive (malignant) diagnosis. A measure of TP and FN is called sensitivity.\n",
    "\n",
    "> Looking at the confusion matrices, logistic regression has fewer false positives (FP=0) compared to the decision tree (FP=5), indicating that logistic regression may be better at minimising false positive predictions, which can be particularly important in a clinical setting where minimising false positives is often desired. This would decrease the number of unnecessary treatments.\n",
    "\n",
    "> The precision-recall curve also shows a higher precision for logistic regression. This indicates that logistic regression may have a better ability to correctly identify positive cases (higher precision) compared to the decision tree.\n",
    "\n",
    "> Considering all these factors, it is recommended to use logistic regression in this clinical setting due to its higher accuracy, lower false positive rate, higher precision, and higher sensitivity. However, specific clinical requirements and computational power should be considered when making a final selection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17055306",
   "metadata": {},
   "source": [
    "## Part 1 - Task 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad16b2dd",
   "metadata": {},
   "source": [
    "**Feature importance for the Logistic Regression model**\n",
    "\n",
    "The coefficients of the features are used to deduce feature importance for the logisitc regression model. The absolute values of the coefficients are used in this plot and are scaled down so they sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de0ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(best_lr, cancer_data, \"Logistic Regression \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "597970b8",
   "metadata": {},
   "source": [
    "**Feature importance for the Decision Tree model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(best_dt,cancer_data, \"Decision Tree\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cde230a",
   "metadata": {},
   "source": [
    "**Discussion about feature importances**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a104402",
   "metadata": {},
   "source": [
    "> Both models exhibit relatively similar levels of feature importance. Both models identify `concave_points_mean` as a highly important feature. On the other hand, `compactness_mean` and `symmetry_mean` are considered less significant in both models.\n",
    "\n",
    "> However, an interesting observation is the varying importance attributed to `radius_mean` between the two models. `radius_mean` displays a strong correlation with `area_mean`. The decision tree model seems to incorporate `area_mean` into its model, which potentially explains the relatively lower importance assigned to `radius_mean` due to redundancy.\n",
    "\n",
    "> This fairly high level of consistency in feature importance is reassuring, as it suggests that these particular features consistently hold valuable information for tumour status prediction. Therefore, they should be given significant consideration when interpreting the models and making decisions based on their outputs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2cde57d",
   "metadata": {},
   "source": [
    "## Part 1 - Task 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c1bee3d",
   "metadata": {},
   "source": [
    "**Creating a scatter plot of the data using the first 2 principle components using PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca25255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No longer using testing and training set in PCA\n",
    "X = cancer_data.iloc[:, 0:-1]\n",
    "y = cancer_data.iloc[:, -1]\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9953b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plotting the PCA\n",
    "pca_plot(X_pca, pca, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "417eb3a4",
   "metadata": {},
   "source": [
    "> The first and second principle components make up 80% of the variance in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be74c257",
   "metadata": {},
   "source": [
    "## Part 1 - Task 4 & 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cdbda91",
   "metadata": {},
   "source": [
    "**Creating a biplot with the variables vectors and the observed data projected on the first two principal components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df6b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cancer_data.columns[0:-1]\n",
    "\n",
    "pca_plot_biplot(X_pca, feature_names, y, pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e5bdef8",
   "metadata": {},
   "source": [
    "> The PCA plot of the dataset reveals important features about the data. Firstly, there is a clear separation between the benign and malignant samples, with many benign samples clustering on the negative side of the 1st principal component, while numerous malignant instances are concentrated on the positive end. A decision boundary can be placed between the two classes. The 1st principle component holds most of the variance of the data, hence, is more important to the categorisation of the data,\n",
    "\n",
    "> Perpendicular to this decision boundary, there is a collection of overlapping features, `perimeter_mean`, `area_mean`, and `radius_mean`. These features are highly correlated due to the circular nature of the cell's nucleus. The PCA plot confirms this correlation, as they exhibit similar loadings. The `texture_mean` feature also aligns with the same direction, but with a lower loading, indicating its relatively lower importance. The direction of these four features suggests a fairly strong correlation between them and the classification of instances, as they are perpendicular to the decision boundary. Also, a significant proportion of their direction lies within the 1st principal component, which accounts for a larger portion of the variance compared to the y-direction (2nd principal axis).\n",
    "\n",
    "> On the other hand, `concave_points_mean`, `concavity_mean`, and `compactness_mean` show the largest component in the direction of the 1st principal component. This indicates their higher importance in categorising the instances.\n",
    "\n",
    "> In the direction of the decision boundary, we observe features such as `fractal_dimension_mean`, `smoothness_mean`, and `symmetry_mean`. These features have larger components in the y-direction, indicating their lesser importance compared to the previously discussed features. Moreover, since they align with the decision boundary direction, they do not provide significant valuable information for the classification.\n",
    "\n",
    "> Comparing the \"important\" features we discussed (`concave_points_mean`, `concavity_mean`, and `compactness_mean`), the fairly important features (`perimeter_mean`, `area_mean`, `radius_mean`, and `texture_mean`), and the \"unimportant features\" (`fractal_dimension_mean`, `smoothness_mean`, and `symmetry_mean`) with the feature importances observed in Task 2, we notice many similarities. The general order of features remains consistent. This indicates that PCA is effective in providing valuable information about features in a dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2d6674e",
   "metadata": {},
   "source": [
    "## Part 1 - Task 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a95c86b5",
   "metadata": {},
   "source": [
    "**Determining the number of features to retain 95% variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = get_num_comps(X_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e2a8831",
   "metadata": {},
   "source": [
    "**Now using 5 features for PCA and repeating task 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda79351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain the desired number of components\n",
    "pca5 = PCA(n_components=num_components)\n",
    "#X_pca5 = pca5.fit_transform(X_train)\n",
    "\n",
    "X_train_PCA = pca5.fit_transform(X_train)\n",
    "X_test_PCA = pca5.transform(X_test)\n",
    "\n",
    "log_reg_PCA = LogisticRegression()\n",
    "\n",
    "log_reg_PCA.fit(X_train_PCA, y_train)\n",
    "y_pred_test_PCA = log_reg_PCA.predict(X_test_PCA)\n",
    "y_pred_train_PCA = log_reg_PCA.predict(X_train_PCA)\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [100, 10, 1.0, 0.1, 0.01],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'class_weight': ['balanced', None],\n",
    "    'solver': ['liblinear', 'saga'], \n",
    "    'max_iter': [10000]\n",
    "}\n",
    "\n",
    "accuracy_train_lr_PCA, accuracy_test_lr_PCA, best_lr_PCA = get_best_clf(log_reg_PCA, param_grid_lr, X_train_PCA, y_train, X_test_PCA, y_test, model_name='logistic regression')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dab4372",
   "metadata": {},
   "source": [
    "**Dimension of new dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the projected dataset\n",
    "num_samples, num_features = X_train_PCA.shape\n",
    "\n",
    "# Print the dimension of the projected dataset\n",
    "print(\"Dimension of the projected traing dataset: {} x {}\".format(num_samples, num_features))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78875da8",
   "metadata": {},
   "source": [
    "> Now, just 5 features are used instead of 10. This makes the model much more computationally efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42286bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_side_by_side(['Training Set', 'Testing Set'], [accuracy_train_lr, accuracy_test_lr], [accuracy_train_lr_PCA, accuracy_test_lr_PCA], rotate=0, title1='Original Logistic Regression Accuracies', title2='Principle Components Logistic Regression', ylabel='Accuracies',ylim=1.05)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84c595c8",
   "metadata": {},
   "source": [
    "> We observe a similar accuracy for both the training and testing sets when using only the top 5 PCA components. This indicates that despite reducing the number of features, the model is still able to capture a significant amount of information necessary for accurate predictions. It is likely that noise in the original dataset was filtered out when using PCA, resulting in the similar accuracies, despite using less information.\n",
    "\n",
    "> By using the top 5 PCA components, we achieve a good balance between feature reduction and preserving the essential characteristics of the data. This reduction in dimensionality can be advantageous in terms of computational efficiency and mitigating the risk of overfitting.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5678dca2",
   "metadata": {},
   "source": [
    "## Part 2 - A Clustering Analysis on Airlines Safety Records"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dc48330",
   "metadata": {},
   "source": [
    "**Defining the functions to be used in this question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc62dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouette(X):\n",
    "    # Define a list to store the silhouette scores\n",
    "    silhouette_scores = []\n",
    "    # Perform K-means clustering for values of K ranging from 2 to 8\n",
    "    for k in range(2, 9):\n",
    "        # Create a K-means instance\n",
    "        kmeans = KMeans(n_clusters=k, random_state=5508, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        labels = kmeans.predict(X)\n",
    "        silhouette_avg = silhouette_score(X, labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Plot the silhouette scores\n",
    "    plt.plot(range(2, 9), silhouette_scores, marker='o')\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Score for K-means Clustering')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_silhouette_and_elbow(X_data):\n",
    "    # Define a list to store the silhouette scores and SSE\n",
    "    silhouette_scores = []\n",
    "    sse = []\n",
    "    \n",
    "    # Perform K-means clustering for values of K ranging from 2 to 8\n",
    "    for k in range(2, 9):\n",
    "        # Create a K-means instance\n",
    "        kmeans = KMeans(n_clusters=k, random_state=5508, n_init=10)\n",
    "        kmeans.fit(X_data)\n",
    "        labels = kmeans.predict(X_data)\n",
    "        \n",
    "        # Calculate silhouette score and append to the list\n",
    "        silhouette_avg = silhouette_score(X_data, labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        \n",
    "        # Calculate SSE and append to the list\n",
    "        sse.append(kmeans.inertia_)\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Plot silhouette scores on the left subplot\n",
    "    ax1.plot(range(2, 9), silhouette_scores, marker='o')\n",
    "    ax1.set_xlabel('Number of Clusters (K)')\n",
    "    ax1.set_ylabel('Silhouette Score')\n",
    "    ax1.set_title('Silhouette Score for K-means Clustering')\n",
    "    \n",
    "    # Plot SSE on the right subplot\n",
    "    ax2.plot(range(2, 9), sse, marker='o')\n",
    "    ax2.set_xlabel('Number of Clusters (K)')\n",
    "    ax2.set_ylabel('Sum of Squared Distances')\n",
    "    ax2.set_title('Elbow Analysis for K-means Clustering')\n",
    "    \n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8ceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_elbow(X):\n",
    "    # Define a list to store the sum of squared distances (SSE)\n",
    "    sse = []\n",
    "    \n",
    "    # Perform K-means clustering for values of K ranging from 1 to 10\n",
    "    for k in range(2, 9):\n",
    "        # Create a K-means instance\n",
    "        kmeans = KMeans(n_clusters=k, random_state=5508, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        \n",
    "        # Append the sum of squared distances to the list\n",
    "        sse.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plot the SSE values\n",
    "    plt.plot(range(2, 9), sse, marker='o')\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('Sum of Squared Distances')\n",
    "    plt.title('Elbow Analysis for K-means Clustering')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_knife(X_data):\n",
    "\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "    for n_clusters in range(2,9):\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(18, 7)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        ax1.set_xlim([-0.1, 1])\n",
    "        ax1.set_ylim([0, len(X_data) + (n_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        clusterer = KMeans(n_clusters=n_clusters, n_init=10, random_state=5508)\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        silhouette_avg = silhouette_score(X_data, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        sample_silhouette_values = silhouette_samples(X_data, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(\n",
    "                np.arange(y_lower, y_upper),\n",
    "                0,\n",
    "                ith_cluster_silhouette_values,\n",
    "                facecolor=color,\n",
    "                edgecolor=color,\n",
    "                alpha=0.7,\n",
    "            )\n",
    "\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            y_lower = y_upper + 10\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        # 2nd Plot showing the actual clusters formed\n",
    "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "        ax2.scatter(\n",
    "            X_data.values[:, 0] , X_data.values[:, 1] , marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "        )\n",
    "\n",
    "        # Labeling the clusters\n",
    "        centers = clusterer.cluster_centers_\n",
    "        ax2.scatter(\n",
    "            centers[:, 0],\n",
    "            centers[:, 1],\n",
    "            marker=\"o\",\n",
    "            c=\"white\",\n",
    "            alpha=1,\n",
    "            s=200,\n",
    "            edgecolor=\"k\",\n",
    "        )\n",
    "\n",
    "        for i, c in enumerate(centers):\n",
    "            ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "        ax2.set_title(\"The visualisation of the clustered data.\")\n",
    "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Silhouette analysis for KMeans clustering on airline safety data with n_clusters = %d\"\n",
    "            % n_clusters,\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6cc588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_knife(X_data):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 2, figsize=(18, 28))\n",
    "    plt.subplots_adjust(hspace=0.3, top=0.95)\n",
    "\n",
    "    for n_clusters, ax1 in zip(range(2, 9), axes.flatten()):\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        ax1.set_xlim([-0.1, 1])\n",
    "        ax1.set_ylim([0, len(X_data) + (n_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        clusterer = KMeans(n_clusters=n_clusters, n_init=10, random_state=5508)\n",
    "        cluster_labels = clusterer.fit_predict(X_data)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        silhouette_avg = silhouette_score(X_data, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        sample_silhouette_values = silhouette_samples(X_data, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(\n",
    "                np.arange(y_lower, y_upper),\n",
    "                0,\n",
    "                ith_cluster_silhouette_values,\n",
    "                facecolor=color,\n",
    "                edgecolor=color,\n",
    "                alpha=0.7,\n",
    "            )\n",
    "\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            y_lower = y_upper + 10\n",
    "\n",
    "        ax1.set_title(f\"The silhouette plot for n_clusters = {n_clusters}\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])\n",
    "        ax1.set_xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "        \n",
    "        # Display silhouette score\n",
    "        ax1.text(0.5, 0.95, f\"Silhouette Score: {silhouette_avg:.3f}\", transform=ax1.transAxes, ha=\"center\", va=\"top\")\n",
    "\n",
    "    # Remove the final empty plot\n",
    "    fig.delaxes(axes[3, 1])\n",
    "\n",
    "    plt.suptitle(\"Silhouette analysis for KMeans clustering on airline safety data\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcd3f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cluster_lists(Z, data, thresh):\n",
    "    # Assign cluster labels based on the chosen cutoff\n",
    "    clusters = fcluster(Z, t=thresh, criterion='distance')\n",
    "\n",
    "    # Create a dictionary to store the states in each cluster\n",
    "    cluster_states = {}\n",
    "\n",
    "    # Iterate over the unique cluster labels\n",
    "    for cluster_label in set(clusters):\n",
    "        # Filter the dataset based on the current cluster label\n",
    "        states = data.loc[clusters == cluster_label, 'State'].tolist()\n",
    "        \n",
    "        # Store the list of states in the cluster dictionary\n",
    "        cluster_states[cluster_label] = states\n",
    "\n",
    "    # Print the states in each cluster\n",
    "    for cluster_label, states in cluster_states.items():\n",
    "        print(f\"Cluster {cluster_label}: {states}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf05a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sns(original_data, vars, labels, title='Pairplot for Features Showing KMeans Clusters'):\n",
    "    colours = sns.color_palette('Set2', n_colors=3)\n",
    "\n",
    "    # Set a larger font size\n",
    "    sns.set(font_scale=1.2, style=\"white\")\n",
    "\n",
    "    # Create a copy of the data with the cluster labels\n",
    "    data_with_clusters = original_data.copy()\n",
    "    data_with_clusters['Cluster'] = labels\n",
    "\n",
    "    # Plot pairplot\n",
    "    g = sns.pairplot(data_with_clusters, vars=vars.columns, hue='Cluster', palette=colours)\n",
    "    \n",
    "    # Set the title\n",
    "    g.fig.suptitle(title, y=1.03)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "347a18f0",
   "metadata": {},
   "source": [
    "## Part 2 - Task 1\n",
    "\n",
    "### Unscaled Data\n",
    "\n",
    "**Silhouette scores and Elbow for K-mean clustering at varying Ks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bfbf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "airline_data = pd.read_csv('airline-safety.csv')\n",
    "\n",
    "# Extract the relevant columns for clustering\n",
    "X = airline_data.iloc[:, 1:]\n",
    "plot_silhouette_and_elbow(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7e727bd",
   "metadata": {},
   "source": [
    "**Silhouette analysis for the unscaled data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09338746",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knife(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05cd2f7e",
   "metadata": {},
   "source": [
    "> Although the unscaled data results in high scores for various values of k, it does not provide accurate and representative information. This is because the avail_seat_km_per_week parameter is several orders of magnitude larger than the other parameters. Therefore, we will not use this data for further analysis. However, based on the results obtained, a k value of 2 or 3 seems to be the most appropriate choice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4142a52c",
   "metadata": {},
   "source": [
    "### Scaled data\n",
    "\n",
    "**Silhouette score and Elbow for K-mean clustering at varying Ks for the scaled data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f72dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "plot_silhouette_and_elbow(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ace89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knife(X_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "801473bd",
   "metadata": {},
   "source": [
    "> The scaled data addresses the issue of varying feature magnitudes. However, the silhouette scores for the scaled data are low, with many instances incorrectly assigned to clusters, as indicated by negative scores in the silhouette analysis. Therefore, using the scaled data is also not a suitable choice in this scenario."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b00d0d8b",
   "metadata": {},
   "source": [
    "### Unscaled data and removing `avail_seat_km_per_week`\n",
    "**Silhouette score and Elbow for K-mean clustering at varying Ks for the uscaled data and dropping the `avail_seat_km_per_week` feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_data_dropped = airline_data.drop('avail_seat_km_per_week', axis=1)\n",
    "# Extract the relevant columns for clustering\n",
    "X_drop_km = airline_data_dropped.iloc[:, 1:]\n",
    "plot_silhouette_and_elbow(X_drop_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knife(X_drop_km)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "910468f5",
   "metadata": {},
   "source": [
    "> The dataset excluding the avail_seat_km_per_week feature yields the best outcome. The silhouette scores remain consistently high, with the maximum score occurring at K=3. This aligns with the elbow observed in the plot. Additionally, the silhouette analysis reveals a relatively even distribution of instances among the three clusters, with the silhouette scores crossing or coming close to each other. These findings collectively indicate a suitable choice for the k value.\n",
    "\n",
    ">  The silhouette score is a measure of how well separated clusters are in a clustering analysis. It quantifies the cohesion and the separation. The silhouette score ranges from -1 to 1. A score of 1 indicates that a data point is well within a cluster, a score of 0 indicates that an instance is between clusters and a score of -1 indicates that an instance may have been assigned to the wrong cluster. The overall silhouette score for a clustering solution is the average silhouette score across all data points. A higher overall silhouette score indicates better-defined and well-separated clusters, while a lower score suggests overlapping or poorly separated clusters.\n",
    "\n",
    "> The unscaled data produces higher scores because the scaling process can sometimes distort the relative distances and distributions of the data points. When the data is unscaled, the clustering algorithm may capture the inherent patterns and structures more accurately, resulting in a more well separated clusters. Scaling the data can potentially flatten or compress the data points, causing overlapping or poorly separated clusters, which may lead to lower silhouette scores.\n",
    "\n",
    "> I have chosen K=3 and will use this value in the rest of the question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efc8e6cd",
   "metadata": {},
   "source": [
    "## Part 2 - Task 2\n",
    "**Applying K-means clustering with K=3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b917ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=5508, n_init=10)\n",
    "labels = kmeans.fit_predict(X_drop_km)\n",
    "\n",
    "# Add the cluster labels to the dataset\n",
    "airline_data['Cluster'] = labels\n",
    "\n",
    "# Display the results\n",
    "print(airline_data[['airline', 'Cluster']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9162d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sns(airline_data_dropped, X_drop_km, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3301d4f",
   "metadata": {},
   "source": [
    "> fatalies 89 an d00 show good separation\n",
    "- when they are overlpapping not so good\n",
    "- more data will make it easier to interpret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10ea965e",
   "metadata": {},
   "source": [
    "## Part 2 - Task 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f160c28",
   "metadata": {},
   "source": [
    "dont htink i should remove any features yet as even though it might improve clustering, the purpose of this model is to determine safety of airlines. If i remove feaures now then i may decrease the overall ability of the model to accuratey categorise based on safety FIX\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2c7574b",
   "metadata": {},
   "source": [
    "**Defining functions to be used in this question**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a82b8c5a",
   "metadata": {},
   "source": [
    "## Part 2 - Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e8fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant columns for clustering (1985-1999)\n",
    "X_85_99 = airline_data[['incidents_85_99', 'fatal_accidents_85_99', 'fatalities_85_99']]\n",
    "\n",
    "# Extract the relevant columns for clustering (2000-2014)\n",
    "X_00_14 = airline_data[['incidents_00_14', 'fatal_accidents_00_14', 'fatalities_00_14']]\n",
    "\n",
    "# Set random state\n",
    "random_state = 5508\n",
    "\n",
    "# Perform K-means clustering (1985-1999)\n",
    "kmeans_85_99 = KMeans(n_clusters=3, random_state=random_state, n_init=10)\n",
    "kmeans_85_99.fit(X_85_99)\n",
    "labels_85_99 = kmeans_85_99.labels_\n",
    "\n",
    "# Perform K-means clustering (2000-2014)\n",
    "kmeans_00_14 = KMeans(n_clusters=3, random_state=random_state, n_init=10)\n",
    "kmeans_00_14.fit(X_00_14)\n",
    "labels_00_14 = kmeans_00_14.labels_\n",
    "\n",
    "plot_sns(X_85_99, X_85_99, labels_85_99, title='Pairplot for Features Showing KMeans Clusters for 1985-1999')\n",
    "plot_sns(X_00_14, X_00_14, labels_00_14, title='Pairplot for Features Showing KMeans Clusters for 2000-2014')\n",
    "\n",
    "#plot_sns_side_by_side(X_85_99, labels_85_99, 'title1', X_00_14, labels_/00_14, 'title2')\n",
    "\n",
    "\n",
    "# # Plot the clusters (1985-1999)\n",
    "# plt.scatter(X_85_99['incidents_85_99'], X_85_99['fatal_accidents_85_99'], c=labels_85_99)\n",
    "# plt.xlabel('Incidents 85-99')\n",
    "# plt.ylabel('Fatal Accidents 85-99')\n",
    "# plt.title('K-means Clustering (1985-1999)')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot the clusters (2000-2014)\n",
    "# plt.scatter(X_00_14['incidents_00_14'], X_00_14['fatal_accidents_00_14'], c=labels_00_14)\n",
    "# plt.xlabel('Incidents 00-14')\n",
    "# plt.ylabel('Fatal Accidents 00-14')\n",
    "# plt.title('K-means Clustering (2000-2014)')\n",
    "# plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9882f28f",
   "metadata": {},
   "source": [
    "FIX\n",
    "need more blue data to make observations\n",
    "\n",
    "200 to 2014\n",
    "much fewer incidents\n",
    "safety prioritised"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12b5951b",
   "metadata": {},
   "source": [
    "## Part 2 - Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a small number to each of the instances to avoid divide-by-0 issues\n",
    "# Add a number to all entries in the DataFrame\n",
    "\n",
    "#add_num = 0.00001\n",
    "add_num = 1\n",
    "\n",
    "# Create a copy of the DataFrame\n",
    "airline_data_copy = airline_data.copy()\n",
    "\n",
    "# Add a small number to the specified columns in the copied DataFrame\n",
    "airline_data_copy['incidents_00_14'] = airline_data_copy['incidents_00_14'] + add_num\n",
    "airline_data_copy['incidents_85_99'] = airline_data_copy['incidents_85_99'] + add_num\n",
    "airline_data_copy['fatal_accidents_00_14'] = airline_data_copy['fatal_accidents_00_14'] + add_num\n",
    "airline_data_copy['fatal_accidents_85_99'] = airline_data_copy['fatal_accidents_85_99'] + add_num\n",
    "airline_data_copy['fatalities_00_14'] = airline_data_copy['fatalities_00_14'] + add_num\n",
    "airline_data_copy['fatalities_85_99'] = airline_data_copy['fatalities_85_99'] + add_num\n",
    "\n",
    "# Defining new features\n",
    "incidents_ratio = airline_data_copy['incidents_00_14']/airline_data_copy['incidents_85_99']\n",
    "fatal_accidents_ratio = airline_data_copy['fatal_accidents_00_14']/airline_data_copy['fatal_accidents_85_99']\n",
    "fatalities_ratio = airline_data_copy['fatalities_00_14']/airline_data_copy['fatalities_85_99']\n",
    "\n",
    "# # Replace NaN and infinity with 0\n",
    "# incidents_ratio = incidents_ratio.replace([np.inf, -np.inf, np.nan], 0)\n",
    "# fatal_accidents_ratio = fatal_accidents_ratio.replace([np.inf, -np.inf, np.nan], 0)\n",
    "# fatalities_ratio = fatalities_ratio.replace([np.inf, -np.inf, np.nan], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c3a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the new features into a new DataFrame\n",
    "new_features = pd.DataFrame({'incidents_ratio': incidents_ratio, 'fatal_accidents_ratio': fatal_accidents_ratio, 'fatalities_ratio': fatalities_ratio})\n",
    "\n",
    "# Set random state\n",
    "random_state = 5508\n",
    "\n",
    "# Scale the features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(new_features)\n",
    "\n",
    "# Perform K-means clustering with the value of K from Task 1\n",
    "k = 3  # Value of K from Task 1\n",
    "kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
    "kmeans.fit(new_features)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "plot_sns(new_features, new_features, labels, title='Pairplot for Features Showing KMeans Clusters for Ratios')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d5c5562",
   "metadata": {},
   "source": [
    "- discuss outliers\n",
    "- oranbge may be an oulier, anomlyl\\might not need a cluster\n",
    "- maybe oragne is significanlty different, need more research\n",
    "- differnt comninations of ratios result in different info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f9598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cluster labels to the original dataset\n",
    "new_features['cluster_labels'] = labels\n",
    "new_features['airline'] = airline_data['airline']\n",
    "\n",
    "# Print the cluster analysis results\n",
    "cluster_analysis = new_features[['airline', 'incidents_ratio', 'fatal_accidents_ratio', 'fatalities_ratio', 'cluster_labels']]\n",
    "print(cluster_analysis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17eaa5d5",
   "metadata": {},
   "source": [
    "only 55 ish instances....\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a77913b",
   "metadata": {},
   "source": [
    "# Part 3 - A clustering analysis on US arrests data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12574026",
   "metadata": {},
   "source": [
    "**Defining the functions to be used in this question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_means(data, set_max_y=True):\n",
    "    \n",
    "    # Group the data by cluster and calculate the mean values\n",
    "    cluster_means = data.groupby('Cluster')['Murder', 'Rape', 'Assault', 'UrbanPop'].mean()\n",
    "\n",
    "    # Get the maximum mean value across all clusters\n",
    "    max_mean = cluster_means.max().max()\n",
    "\n",
    "    # Plotting three bar plots side by side\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        cluster_data = cluster_means.iloc[i]\n",
    "        ax.bar(cluster_data.index, cluster_data.values)\n",
    "        ax.set_title(f'Cluster {i+1}')\n",
    "        ax.set_ylabel('Mean Value')\n",
    "        ax.set_xlabel('Variable')\n",
    "\n",
    "        if set_max_y==True:\n",
    "            ax.set_ylim([0, max_mean])  # Set a common y-axis range\n",
    "\n",
    "        if set_max_y==False:\n",
    "            ax.set_ylim([-0.8, 1.5])  # Set a common y-axis range\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f837bf9c",
   "metadata": {},
   "source": [
    "## Part 3 - Task 1\n",
    "\n",
    "**Performing hierarchical clustering with complete linkage and Euclidean distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62b57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_US = pd.read_csv('USArrests.csv')\n",
    "X_US = data_US.iloc[:, 1:].values  # Selecting columns 1 to 4 (Murder, Assault, Rape, UrbanPop)\n",
    "\n",
    "# Calculate the linkage matrix using complete linkage and Euclidean distance\n",
    "Z = linkage(X_US, method='complete', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the dendrogram\n",
    "\n",
    "thresh = 130\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(Z, labels=data_US.iloc[:, 0].values, leaf_rotation=90, color_threshold=thresh)\n",
    "plt.xlabel('States')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.axhline(y=thresh, color='r', linestyle='--')  # Add the horizontal line at height 130\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ec1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign cluster labels based on the chosen cutoff\n",
    "clusters = fcluster(Z, t=thresh, criterion='distance')\n",
    "\n",
    "# Add the cluster labels to the dataset\n",
    "data_US['Cluster'] = clusters\n",
    "\n",
    "# Print the clusters\n",
    "#print(data_US[['State', 'Cluster']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9f460a5",
   "metadata": {},
   "source": [
    "**Displaying the states that belong to each cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0db755",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cluster_lists(Z, data_US, thresh=130)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e2b7f65",
   "metadata": {},
   "source": [
    "**Plotting the mean variables of each cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a8f680",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_means(data_US)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ba75a1a",
   "metadata": {},
   "source": [
    "> The states belonging to each cluster are displayed in the dendrogram and the above list. In the dendrogram, we observe that clusters merge at a similar height, indicating a comparable level of dissimilarity between the merged clusters. The height of each vertical line represents the distance between the merged clusters, with longer lines indicating greater dissimilarity.\n",
    "\n",
    "> In this dendrogram, the vertical lines within the clusters remain relatively short, suggesting higher similarity within each cluster. In contrast, the lines are longer at the cutoff point, indicating greater dissimilarity between the three clusters.\n",
    "\n",
    "\n",
    "> Additionally, examining the mean values of the crimes for each cluster reveals a distinct pattern. Cluster 1 exhibits the highest average rates for each crime, followed by cluster 2 and then cluster 3. This pattern suggests that the centroids of these clusters are likely influenced by these mean values, with cluster 1 representing states with relatively higher crime rates and cluster 3 representing states with relatively lower crime rates\n",
    "\n",
    "> The mean urban population amongst the 3 clusters remains relatively the same. This indicates a consistent distribution of urban population across the states within each cluster. This suggests that the clustering algorithm primarily considered other variables (such as Murder, Rape, and Assault rates) rather than the urban population percentage when forming the clusters.\n",
    "\n",
    "> With additional data, it would be possible to explore and compare various features that could further quantify the clustering of states. Some potential features to consider could include the cost of living, wages, age distribution, population density, and other relevant socio-economic factors. This would enable a deeper exploration of the relationships between crime rates, urban population, and these additional socio-economic features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd31d99c",
   "metadata": {},
   "source": [
    "## Part 3 - Task 2\n",
    "\n",
    "**Reapting task 1 after scaling the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dea533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the variables to have zero mean and unit standard deviation\n",
    "scaler = StandardScaler()\n",
    "X_US_scaled = scaler.fit_transform(X_US)\n",
    "\n",
    "# Calculate the linkage matrix using complete linkage and Euclidean distance\n",
    "Z_scaled = linkage(X_US_scaled, method='complete', metric='euclidean')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(Z_scaled, labels=data_US.iloc[:, 0].values, leaf_rotation=90, color_threshold=4.45)\n",
    "plt.xlabel('States')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.title('Hierarchical Clustering Dendrogram (Scaled Variables)')\n",
    "plt.axhline(y=4.45, color='r', linestyle='--')  # Add the horizontal line at height 130\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7a70826",
   "metadata": {},
   "source": [
    "**Displaying the states that belong to each cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0cdded",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_US_scaled_pd = pd.DataFrame(X_US_scaled, columns=['Murder', 'Assault', 'UrbanPop', 'Rape'])\n",
    "\n",
    "# Assign cluster labels based on the chosen cutoff\n",
    "clusters_scaled = fcluster(Z_scaled, t=4.45, criterion='distance')\n",
    "\n",
    "# Add the cluster labels to the dataset\n",
    "X_US_scaled_pd['Cluster'] = clusters_scaled\n",
    "X_US_scaled_pd['State']=data_US['State']\n",
    "\n",
    "display_cluster_lists(Z_scaled, X_US_scaled_pd, thresh=4.45)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46a7443c",
   "metadata": {},
   "source": [
    "**Plotting the mean variables of each cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936cc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_means(X_US_scaled_pd, set_max_y=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9dacdd99",
   "metadata": {},
   "source": [
    "> Scaling the variables has the effect of standardising the data, ensuring that each variable has the same scale and removing any biases introduced by differences in the original ranges. For this data, `Murder`, `Assault` and `Rape` were measured per 100,000. Whereas the `UrbanPop` was a percentage. It is therefore necessary to scale the data to allow for better data implementation. Scaling eliminates the potential dominance of variables with larger magnitudes, which in this case is `Murder`, `Assault` and `Rape`.\n",
    "\n",
    "> Looking at the dendrogram, we can see that the vertical lines are longer. This indicates a larger dissimilarity between clusters at different levels. This can be attributed to the scaling process, which equalises the importance of each variable in the clustering analysis. As a result, variables with larger magnitudes, such as `Assault` or `Murder`, may contribute more to the dissimilarity between clusters compared to variables with smaller magnitudes, such as `UrbanPop`.\n",
    "\n",
    "> It is also seen that one cluster contains more states than the other two. Looking at the means of the variables, we see that this cluster has notably lower crime rates than the other clusters. It seems that many states fall into this 'low crime rate' category. Although, this is a comparison to other state. It is equally likely that there is a smaller subset of states which has far higher crime rates.\n",
    "\n",
    "> Looking at the averages of the means, we observe different characteristics in the scaled data compared to the unscaled data. Firstly, in cluster 1, we notice a low rate of crime. Interestingly, there appears to be a correlation between crime and urban population. States with a moderately sized urban population tend to have lower crime rates in this cluster. On the other hand, cluster 2 comprises states with a significantly higher urban population, and correspondingly, a higher rate of crime. In cluster 3, we observe states with a smaller urban population, yet a relatively high rate of crime.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78f18cb4",
   "metadata": {},
   "source": [
    "## Part 3 - Task 3\n",
    "\n",
    "**Performing a PCA on the data and using it to perform hierarchical clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3240c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "PC_scores = pca.fit_transform(X_US_scaled)\n",
    "\n",
    "# Calculate the linkage matrix using complete linkage and Euclidean distance on the first two principal components\n",
    "Z_PCA = linkage(PC_scores, method='complete', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b9903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the dendrogram at a height that results in three distinct clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(Z_PCA, labels=data_US.iloc[:, 0].values, leaf_rotation=90, color_threshold=4.2)\n",
    "plt.xlabel('States')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.title('Hierarchical Clustering Dendrogram (First Two Principal Components)')\n",
    "plt.axhline(y=4.2, color='r', linestyle='--')  # Add the horizontal line at height 130\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e90c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign cluster labels based on the chosen cutoff\n",
    "clusters_PCA = fcluster(Z_PCA, t=4.2, criterion='distance')\n",
    "\n",
    "# Add the cluster labels to the dataset\n",
    "# data_US['Cluster'] = clusters\n",
    "\n",
    "# Plot the scatterplot of the first two principal components with different colors for each cluster\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(PC_scores[:, 0], PC_scores[:, 1], c=clusters_PCA, cmap='viridis')\n",
    "plt.xlabel('1st Principal Component')\n",
    "plt.ylabel('2nd Principal Component')\n",
    "plt.title('Scatterplot of First Two Principal Components (Clustered)')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa93eec4",
   "metadata": {},
   "source": [
    "> In this task, PCA was applied to the data to obtain the first two principal components. These components were then used to create a dendrogram through hierarchical clustering with complete linkage and Euclidean distance. When comparing this to the dendrogram in task 2, we find that there are many similarities. The height at which the dendrogram needs to be cut to obtain three distinct clusters is relatively consistent, with 4.2 in this PCA-based clustering and 4.45 in the previous clustering performed on the scaled data.\n",
    "\n",
    "> Again, the dendrogram reveals an imbalanced cluster distribution, where one cluster appears significantly larger than the other two, which is consistent with the previous analysis. This pattern is also reflected in the scatterplot of the first two principal components, where one cluster is noticeably larger than the other two.\n",
    "\n",
    "> The clear separation observed between the clusters in the PCA scatterplot shows that the clusters are well defined. THis indicates that the PCA has captured meaningful patterns in the data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb94df0d",
   "metadata": {},
   "source": [
    "## Part 3 - Task 4\n",
    "\n",
    "**Repeating the analysis of Task 3 using the K-means clustering (with K=3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd60cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute group means from hierarchical clustering\n",
    "group_means = pd.DataFrame(PC_scores).groupby(clusters).mean()\n",
    "\n",
    "# Use group means as initial centroids for K-means clustering\n",
    "kmeans_US = KMeans(n_clusters=3, init=group_means.values, n_init=1, random_state=5508)\n",
    "kmeans_US.fit(PC_scores)\n",
    "\n",
    "# Get the cluster labels from K-means clustering\n",
    "kmeans_clusters = kmeans_US.labels_\n",
    "\n",
    "# Plot the scatterplot of the first two principal components with different colors for each K-means cluster\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(PC_scores[:, 0], PC_scores[:, 1], c=kmeans_clusters, cmap='viridis')\n",
    "plt.scatter(group_means.iloc[:, 0], group_means.iloc[:, 1], c='red', marker='x', s=100, label='Initial Centroids')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Scatterplot of First Two Principal Components (K-means Clustering)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cda63b3c",
   "metadata": {},
   "source": [
    "> The results here are identical to that in task 3. Although, in situations where there is much more data, this method would likely be more beneficial. By setting the centroids to the means of the PCA data, it allows the model to fine-tune the clustering, and not waste time attempting to find where a cluster actually is. This way we can also pre-define the clusters and control the way the clustering occurs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
